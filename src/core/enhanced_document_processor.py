# Generated by Copilot
from typing import List, Optional, Dict, Any
import os
import json
from datetime import datetime

from ..models.document import Document, DocumentType, ProcessingStatus, ProcessingTier
from ..core.file_handler import FileHandler
from ..quality.quality_assessor import QualityAssessor
from ..ai.donut_ocr import DonutOCREngine
from ..utils.config import Config
from ..utils.logger import Logger

class EnhancedDocumentProcessor:
    """
    Phase 2 Document Processor with Donut OCR Integration
    Replaces paid APIs with free HuggingFace transformers
    """
    
    def __init__(self):
        self.logger = Logger(__name__)
        self.config = Config()
        self.file_handler = FileHandler()
        self.quality_assessor = QualityAssessor()
        
        # Phase 2: Initialize Donut OCR engine
        try:
            self.donut_ocr = DonutOCREngine()
            self.ocr_available = True
            self.logger.info("Donut OCR engine initialized successfully")
        except Exception as e:
            self.logger.warning(f"Donut OCR initialization failed: {e}")
            self.ocr_available = False
        
        # Processing statistics
        self.processing_stats = {
            'total_processed': 0,
            'high_quality_auto': 0,
            'medium_quality_enhanced': 0,
            'low_quality_manual': 0,
            'donut_ocr_processed': 0,
            'processing_errors': 0
        }
    
    def process_document(self, file_path: str, original_filename: Optional[str] = None) -> Document:
        """
        Phase 2 main entry point for document processing with Donut OCR
        """
        try:
            if original_filename is None:
                original_filename = os.path.basename(file_path)
            
            self.logger.info(f"Starting Phase 2 document processing: {original_filename}")
            
            # Step 1: Validate and save file (Phase 1)
            is_valid, error_msg = self.file_handler.validate_file(file_path)
            if not is_valid:
                raise ValueError(f"File validation failed: {error_msg}")
            
            # Create document object
            document = self.file_handler.save_uploaded_file(file_path, original_filename)
            document.status = ProcessingStatus.UPLOADED
            
            # Step 2: Convert to images if needed (Phase 1)
            image_paths = self.file_handler.convert_to_images(document)
            
            # Step 3: Quality assessment (Phase 1)
            document = self.quality_assessor.assess_document_quality(document)
            document.status = ProcessingStatus.QUALITY_ASSESSED
            
            # Step 4: Smart routing based on quality (Enhanced Phase 2)
            document = self._route_document_phase2(document)
            
            # Step 5: Document classification with Donut (Phase 2)
            document = self._classify_document_donut(document)
            
            # Step 6: Data extraction with Donut OCR (Phase 2)
            if self.ocr_available and document.quality_metrics.overall_score >= 30:
                document = self._extract_data_donut(document)
                document.status = ProcessingStatus.PROCESSED
            else:
                document.status = ProcessingStatus.REQUIRES_HUMAN_REVIEW
            
            # Step 7: Post-processing and validation (Phase 2)
            document = self._post_process_results(document)
            
            # Update statistics
            self._update_processing_stats(document)
            
            # Step 8: Save processing results
            self._save_processing_results(document)
            
            self.logger.info(f"Phase 2 document processing completed: {document.id}", {
                'quality_score': document.quality_metrics.overall_score,
                'processing_tier': document.processing_tier.value,
                'document_type': document.document_type.value if document.document_type else 'unknown',
                'ocr_processed': len(document.processing_results) > 0
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error processing document {original_filename}: {e}")
            # Create error document if possible
            if 'document' in locals():
                document.status = ProcessingStatus.FAILED
                document.add_processing_step('processing_error', {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                self.processing_stats['processing_errors'] += 1
                return document
            else:
                raise
    
    def _route_document_phase2(self, document: Document) -> Document:
        """Enhanced routing with Donut OCR integration"""
        try:
            quality_score = document.quality_metrics.overall_score
            
            if quality_score >= self.config.QUALITY_THRESHOLD_HIGH:
                # High quality - Donut OCR processing
                processing_path = "high_quality_donut_ocr"
                next_steps = ["donut_classification", "donut_extraction", "validation"]
                estimated_cost = 0.0  # Free!
                document.processing_tier = ProcessingTier.HIGH_QUALITY
                
                self.processing_stats['high_quality_auto'] += 1
                
            elif quality_score >= self.config.QUALITY_THRESHOLD_MEDIUM:
                # Medium quality - Enhanced Donut processing
                processing_path = "medium_quality_donut_enhanced"
                next_steps = ["image_enhancement", "donut_classification", "donut_extraction", "validation"]
                estimated_cost = 0.0  # Still free!
                document.processing_tier = ProcessingTier.MEDIUM_QUALITY
                
                self.processing_stats['medium_quality_enhanced'] += 1
                
            else:
                # Low quality - Best effort with human fallback
                processing_path = "low_quality_donut_attempt"
                next_steps = ["image_enhancement", "donut_best_effort", "human_verification"]
                estimated_cost = 0.0  # Free, but may need human review
                document.processing_tier = ProcessingTier.LOW_QUALITY
                
                self.processing_stats['low_quality_manual'] += 1
            
            # Add routing information to document
            document.add_processing_step('phase2_smart_routing', {
                'quality_score': quality_score,
                'processing_path': processing_path,
                'next_steps': next_steps,
                'estimated_cost': estimated_cost,
                'ocr_engine': 'donut_transformer',
                'routing_timestamp': datetime.now().isoformat()
            })
            
            self.logger.debug(f"Document routed to {processing_path}", {
                'document_id': document.id,
                'quality_score': quality_score,
                'estimated_cost': estimated_cost
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error routing document: {e}")
            # Default to manual review on error
            document.processing_tier = ProcessingTier.LOW_QUALITY
            document.status = ProcessingStatus.REQUIRES_HUMAN_REVIEW
            return document
    
    def _classify_document_donut(self, document: Document) -> Document:
        """
        Enhanced document classification using Donut's understanding capabilities
        """
        try:
            if not self.ocr_available:
                return self._classify_document_basic(document)
            
            self.logger.info(f"Classifying document with Donut: {document.id}")
            
            # Use filename and basic content analysis for classification
            filename = document.original_filename.lower()
            
            # Enhanced classification logic
            if any(keyword in filename for keyword in ['cv', 'resume', 'curriculum']):
                document.document_type = DocumentType.CV_RESUME
                confidence = 85
            elif any(keyword in filename for keyword in ['passport', 'travel']):
                document.document_type = DocumentType.PASSPORT
                confidence = 80
            elif any(keyword in filename for keyword in ['certificate', 'cert', 'diploma']):
                document.document_type = DocumentType.EDUCATIONAL_CERTIFICATE
                confidence = 75
            elif any(keyword in filename for keyword in ['experience', 'employment', 'work']):
                document.document_type = DocumentType.EXPERIENCE_LETTER
                confidence = 70
            elif any(keyword in filename for keyword in ['id', 'cnic', 'national']):
                document.document_type = DocumentType.NATIONAL_ID
                confidence = 75
            else:
                # Use Donut for content-based classification if needed
                document.document_type = DocumentType.OTHER
                confidence = 50
            
            # Add classification step
            document.add_processing_step('donut_classification', {
                'classified_as': document.document_type.value,
                'method': 'filename_analysis_enhanced',
                'confidence': confidence,
                'timestamp': datetime.now().isoformat()
            })
            
            self.logger.debug(f"Document classified as {document.document_type.value}", {
                'document_id': document.id,
                'confidence': confidence
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error classifying document with Donut: {e}")
            return self._classify_document_basic(document)
    
    def _extract_data_donut(self, document: Document) -> Document:
        """
        Extract structured data using Donut OCR
        """
        try:
            if not self.ocr_available:
                self.logger.warning("Donut OCR not available, skipping extraction")
                return document
            
            self.logger.info(f"Extracting data with Donut OCR: {document.id}")
            
            # Process document with Donut
            document = self.donut_ocr.process_document(document)
            
            # Update processing statistics
            self.processing_stats['donut_ocr_processed'] += 1
            
            # Add processing step
            document.add_processing_step('donut_data_extraction', {
                'extraction_method': 'donut_transformer',
                'fields_extracted': len(document.extracted_data) if document.extracted_data else 0,
                'processing_cost': 0.0,
                'timestamp': datetime.now().isoformat()
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error extracting data with Donut: {e}")
            document.add_processing_step('donut_extraction_error', {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
            return document
    
    def _post_process_results(self, document: Document) -> Document:
        """
        Post-process and validate extracted data
        """
        try:
            if not document.extracted_data:
                return document
            
            # Validate extracted data quality
            validation_results = self._validate_extracted_data(document)
            
            # Add validation results
            document.add_processing_step('data_validation', {
                'validation_results': validation_results,
                'timestamp': datetime.now().isoformat()
            })
            
            # Determine if human review is needed based on confidence
            avg_confidence = sum(field.confidence for field in document.extracted_data.values()) / len(document.extracted_data)
            
            if avg_confidence < 70:
                document.status = ProcessingStatus.REQUIRES_HUMAN_REVIEW
                document.add_processing_step('low_confidence_flagged', {
                    'average_confidence': avg_confidence,
                    'threshold': 70,
                    'timestamp': datetime.now().isoformat()
                })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error in post-processing: {e}")
            return document
    
    def _validate_extracted_data(self, document: Document) -> Dict[str, Any]:
        """
        Validate extracted data for completeness and accuracy
        """
        validation_results = {
            'total_fields': len(document.extracted_data),
            'high_confidence_fields': 0,
            'medium_confidence_fields': 0,
            'low_confidence_fields': 0,
            'validation_score': 0.0
        }
        
        for field_name, field_data in document.extracted_data.items():
            if field_data.confidence >= 80:
                validation_results['high_confidence_fields'] += 1
            elif field_data.confidence >= 60:
                validation_results['medium_confidence_fields'] += 1
            else:
                validation_results['low_confidence_fields'] += 1
        
        # Calculate validation score
        total_fields = validation_results['total_fields']
        if total_fields > 0:
            high_weight = validation_results['high_confidence_fields'] * 1.0
            medium_weight = validation_results['medium_confidence_fields'] * 0.7
            low_weight = validation_results['low_confidence_fields'] * 0.3
            
            validation_results['validation_score'] = (high_weight + medium_weight + low_weight) / total_fields * 100
        
        return validation_results
    
    def _classify_document_basic(self, document: Document) -> Document:
        """Fallback basic classification method"""
        filename = document.original_filename.lower()
        
        if any(keyword in filename for keyword in ['cv', 'resume']):
            document.document_type = DocumentType.CV_RESUME
        elif 'passport' in filename:
            document.document_type = DocumentType.PASSPORT
        elif 'certificate' in filename:
            document.document_type = DocumentType.EDUCATIONAL_CERTIFICATE
        else:
            document.document_type = DocumentType.OTHER
        
        document.add_processing_step('basic_classification_fallback', {
            'classified_as': document.document_type.value,
            'confidence': 60,
            'timestamp': datetime.now().isoformat()
        })
        
        return document
    
    def _update_processing_stats(self, document: Document):
        """Update processing statistics"""
        self.processing_stats['total_processed'] += 1
    
    def _save_processing_results(self, document: Document):
        """Save processing results to file"""
        try:
            results_file = os.path.join(self.config.PROCESSED_FOLDER, f"{document.id}_results.json")
            
            results_data = {
                'document_id': document.id,
                'original_filename': document.original_filename,
                'processing_timestamp': datetime.now().isoformat(),
                'quality_score': document.quality_metrics.overall_score if document.quality_metrics else 0,
                'document_type': document.document_type.value if document.document_type else 'unknown',
                'processing_tier': document.processing_tier.value if document.processing_tier else 'unknown',
                'status': document.status.value,
                'extracted_data': {k: {'value': v.value, 'confidence': v.confidence} 
                                 for k, v in document.extracted_data.items()} if document.extracted_data else {},
                'processing_history': document.processing_history,
                'processing_cost': document.processing_cost
            }
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug(f"Results saved: {results_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving processing results: {e}")
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get enhanced processing statistics"""
        stats = self.processing_stats.copy()
        
        # Calculate percentages
        total = stats['total_processed']
        if total > 0:
            stats['percentages'] = {
                'high_quality_auto': (stats['high_quality_auto'] / total) * 100,
                'medium_quality_enhanced': (stats['medium_quality_enhanced'] / total) * 100,
                'low_quality_manual': (stats['low_quality_manual'] / total) * 100,
                'donut_ocr_success': (stats['donut_ocr_processed'] / total) * 100,
                'error_rate': (stats['processing_errors'] / total) * 100
            }
        
        return {
            'stats': stats,
            'ocr_engine': 'donut_transformer',
            'phase': 'Phase 2 - Donut OCR',
            'timestamp': datetime.now().isoformat()
        }
    
    def cleanup_resources(self):
        """Clean up resources, especially GPU memory"""
        if hasattr(self, 'donut_ocr') and self.donut_ocr:
            self.donut_ocr.cleanup_models()
            self.logger.info("Donut OCR resources cleaned up")
