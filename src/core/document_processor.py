# Generated by Copilot
from typing import List, Optional, Dict, Any
import os
import json
from datetime import datetime

from ..models.document import Document, DocumentType, ProcessingStatus, ProcessingTier
from ..core.file_handler import FileHandler
from ..quality.quality_assessor import QualityAssessor
from ..utils.config import Config
from ..utils.logger import Logger

class DocumentProcessor:
    """Main document processing orchestrator for Phase 1"""
    
    def __init__(self):
        self.logger = Logger(__name__)
        self.config = Config()
        self.file_handler = FileHandler()
        self.quality_assessor = QualityAssessor()
        
        # Processing statistics
        self.processing_stats = {
            'total_processed': 0,
            'high_quality_auto': 0,
            'medium_quality_enhanced': 0,
            'low_quality_manual': 0,
            'processing_errors': 0
        }
    
    def process_document(self, file_path: str, original_filename: Optional[str] = None) -> Document:
        """
        Main entry point for document processing
        Phase 1 implementation focuses on file handling, quality assessment, and routing
        """
        try:
            if original_filename is None:
                original_filename = os.path.basename(file_path)
            
            self.logger.info(f"Starting document processing: {original_filename}")
            
            # Step 1: Validate and save file
            is_valid, error_msg = self.file_handler.validate_file(file_path)
            if not is_valid:
                raise ValueError(f"File validation failed: {error_msg}")
            
            # Create document object
            document = self.file_handler.save_uploaded_file(file_path, original_filename)
            document.status = ProcessingStatus.UPLOADED
            
            # Step 2: Convert to images if needed (for quality assessment)
            image_paths = self.file_handler.convert_to_images(document)
            
            # Step 3: Quality assessment
            document = self.quality_assessor.assess_document_quality(document)
            document.status = ProcessingStatus.QUALITY_ASSESSED
            
            # Step 4: Smart routing based on quality
            document = self._route_document(document)
            
            # Step 5: Basic classification (Phase 1 - simple file extension based)
            document = self._classify_document_basic(document)
            
            # Update statistics
            self._update_processing_stats(document)
            
            # Step 6: Save processing results
            self._save_processing_results(document)
            
            self.logger.info(f"Document processing completed: {document.id}", {
                'quality_score': document.quality_metrics.overall_score,
                'processing_tier': document.processing_tier.value,
                'document_type': document.document_type.value if document.document_type else 'unknown'
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error processing document {original_filename}: {e}")
            # Create error document if possible
            if 'document' in locals():
                document.status = ProcessingStatus.FAILED
                document.add_processing_step('processing_error', {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                self.processing_stats['processing_errors'] += 1
                return document
            else:
                raise
    
    def _route_document(self, document: Document) -> Document:
        """Route document based on quality score"""
        try:
            quality_score = document.quality_metrics.overall_score
            
            if quality_score >= self.config.QUALITY_THRESHOLD_HIGH:
                # High quality - fully automated processing
                processing_path = "high_quality_automated"
                next_steps = ["huggingface_donut_ocr", "automated_extraction"]
                estimated_cost = self.config.get_ocr_cost('huggingface_donut')
                
                self.processing_stats['high_quality_auto'] += 1
                
            elif quality_score >= self.config.QUALITY_THRESHOLD_MEDIUM:
                # Medium quality - enhanced processing
                processing_path = "medium_quality_enhanced"
                next_steps = ["image_enhancement", "google_vision_ocr", "validation"]
                estimated_cost = self.config.get_ocr_cost('google_vision')
                
                self.processing_stats['medium_quality_enhanced'] += 1
                
            else:
                # Low quality - human verification required
                processing_path = "low_quality_manual"
                next_steps = ["image_enhancement", "best_effort_ocr", "human_verification"]
                estimated_cost = self.config.get_ocr_cost('openai_gpt4v')
                
                self.processing_stats['low_quality_manual'] += 1
                document.status = ProcessingStatus.REQUIRES_HUMAN_REVIEW
            
            # Add routing information to document
            document.add_processing_step('smart_routing', {
                'quality_score': quality_score,
                'processing_path': processing_path,
                'next_steps': next_steps,
                'estimated_cost': estimated_cost,
                'routing_timestamp': datetime.now().isoformat()
            })
            
            self.logger.debug(f"Document routed to {processing_path}", {
                'document_id': document.id,
                'quality_score': quality_score,
                'estimated_cost': estimated_cost
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error routing document: {e}")
            # Default to manual review on error
            document.processing_tier = ProcessingTier.LOW_QUALITY
            document.status = ProcessingStatus.REQUIRES_HUMAN_REVIEW
            return document
    
    def _classify_document_basic(self, document: Document) -> Document:
        """
        Basic document classification for Phase 1
        More sophisticated AI-based classification will be implemented in Phase 2
        """
        try:
            filename = document.original_filename.lower()
            
            # Simple keyword-based classification
            if any(keyword in filename for keyword in ['cv', 'resume', 'curriculum']):
                document.document_type = DocumentType.CV_RESUME
            elif any(keyword in filename for keyword in ['passport', 'pp']):
                document.document_type = DocumentType.PASSPORT
            elif any(keyword in filename for keyword in ['certificate', 'cert', 'diploma', 'degree']):
                document.document_type = DocumentType.EDUCATION_CERTIFICATE
            elif any(keyword in filename for keyword in ['experience', 'employment', 'work']):
                document.document_type = DocumentType.EXPERIENCE_LETTER
            elif any(keyword in filename for keyword in ['cnic', 'id', 'identity', 'national']):
                document.document_type = DocumentType.NATIONAL_ID
            elif any(keyword in filename for keyword in ['medical', 'health']):
                document.document_type = DocumentType.MEDICAL_CERTIFICATE
            elif any(keyword in filename for keyword in ['training', 'course']):
                document.document_type = DocumentType.TRAINING_CERTIFICATE
            elif any(keyword in filename for keyword in ['reference', 'recommendation']):
                document.document_type = DocumentType.REFERENCE_LETTER
            else:
                document.document_type = DocumentType.UNKNOWN
            
            # Add classification step
            document.add_processing_step('basic_classification', {
                'classified_as': document.document_type.value,
                'method': 'keyword_matching',
                'confidence': 60,  # Low confidence for basic classification
                'requires_ai_verification': True
            })
            
            self.logger.debug(f"Document classified as {document.document_type.value}", {
                'document_id': document.id,
                'filename': document.original_filename
            })
            
            return document
            
        except Exception as e:
            self.logger.error(f"Error classifying document: {e}")
            document.document_type = DocumentType.UNKNOWN
            return document
    
    def _update_processing_stats(self, document: Document):
        """Update processing statistics"""
        self.processing_stats['total_processed'] += 1
        
        # Log current statistics periodically
        if self.processing_stats['total_processed'] % 10 == 0:
            self.logger.info("Processing statistics update", self.processing_stats)
    
    def _save_processing_results(self, document: Document):
        """Save processing results to file (Phase 1 - JSON files)"""
        try:
            # Create results directory
            results_dir = os.path.join(self.config.PROCESSED_FOLDER, 'results')
            os.makedirs(results_dir, exist_ok=True)
            
            # Save document metadata
            result_file = os.path.join(results_dir, f"{document.id}.json")
            with open(result_file, 'w') as f:
                json.dump(document.to_dict(), f, indent=2)
            
            self.logger.debug(f"Processing results saved: {result_file}")
            
        except Exception as e:
            self.logger.error(f"Error saving processing results: {e}")
    
    def batch_process_folder(self, folder_path: str) -> List[Document]:
        """Process all supported files in a folder"""
        try:
            processed_documents = []
            
            self.logger.info(f"Starting batch processing of folder: {folder_path}")
            
            # Get all supported files
            supported_files = []
            for filename in os.listdir(folder_path):
                file_path = os.path.join(folder_path, filename)
                if os.path.isfile(file_path):
                    file_extension = os.path.splitext(filename)[1].lower()
                    if self.config.is_supported_format(file_extension):
                        supported_files.append((file_path, filename))
            
            self.logger.info(f"Found {len(supported_files)} supported files")
            
            # Process each file
            for file_path, filename in supported_files:
                try:
                    document = self.process_document(file_path, filename)
                    processed_documents.append(document)
                    
                except Exception as e:
                    self.logger.error(f"Error processing file {filename}: {e}")
                    continue
            
            # Generate batch summary
            self._generate_batch_summary(processed_documents, folder_path)
            
            self.logger.info(f"Batch processing completed. Processed {len(processed_documents)} documents")
            
            return processed_documents
            
        except Exception as e:
            self.logger.error(f"Error in batch processing: {e}")
            return []
    
    def _generate_batch_summary(self, documents: List[Document], folder_path: str):
        """Generate summary report for batch processing"""
        try:
            summary = {
                'batch_info': {
                    'folder_path': folder_path,
                    'total_documents': len(documents),
                    'processed_at': datetime.now().isoformat()
                },
                'quality_distribution': {
                    'high_quality': len([d for d in documents if d.processing_tier == ProcessingTier.HIGH_QUALITY]),
                    'medium_quality': len([d for d in documents if d.processing_tier == ProcessingTier.MEDIUM_QUALITY]),
                    'low_quality': len([d for d in documents if d.processing_tier == ProcessingTier.LOW_QUALITY])
                },
                'document_types': {},
                'requires_human_review': len([d for d in documents if d.status == ProcessingStatus.REQUIRES_HUMAN_REVIEW]),
                'processing_errors': len([d for d in documents if d.status == ProcessingStatus.FAILED]),
                'average_quality_score': sum([d.quality_metrics.overall_score for d in documents if d.quality_metrics]) / len(documents) if documents else 0
            }
            
            # Count document types
            for doc in documents:
                if doc.document_type:
                    doc_type = doc.document_type.value
                    summary['document_types'][doc_type] = summary['document_types'].get(doc_type, 0) + 1
            
            # Save summary
            summary_file = os.path.join(
                self.config.PROCESSED_FOLDER, 
                'results', 
                f"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            with open(summary_file, 'w') as f:
                json.dump(summary, f, indent=2)
            
            self.logger.info(f"Batch summary saved: {summary_file}", summary)
            
        except Exception as e:
            self.logger.error(f"Error generating batch summary: {e}")
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get current processing statistics"""
        return {
            'stats': self.processing_stats,
            'config': {
                'quality_thresholds': {
                    'high': self.config.QUALITY_THRESHOLD_HIGH,
                    'medium': self.config.QUALITY_THRESHOLD_MEDIUM,
                    'low': self.config.QUALITY_THRESHOLD_LOW
                },
                'supported_formats': self.config.get_supported_formats(),
                'max_file_size_mb': self.config.MAX_FILE_SIZE_MB
            }
        }
    
    def get_document_by_id(self, document_id: str) -> Optional[Document]:
        """Retrieve document by ID (Phase 1 - from JSON files)"""
        try:
            result_file = os.path.join(
                self.config.PROCESSED_FOLDER, 
                'results', 
                f"{document_id}.json"
            )
            
            if os.path.exists(result_file):
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                # Note: This is a simplified reconstruction
                # In Phase 2, this would be replaced with database queries
                document = Document(data['file_path'], data['original_filename'])
                document.id = data['id']
                # ... (reconstruct other fields as needed)
                
                return document
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error retrieving document {document_id}: {e}")
            return None
