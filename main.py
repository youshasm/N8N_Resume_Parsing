# Generated by Copilot
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import os
import tempfile
import shutil
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from datetime import datetime

# Unified imports - Enhanced processor with Phase 1 fallback built-in
from src.core.enhanced_document_processor import EnhancedDocumentProcessor
from src.core.document_processor import DocumentProcessor
from src.models.document import ProcessingTier
from src.utils.config import Config
from src.utils.logger import Logger
from src.utils.redis_client import redis_client  # Now using in-memory storage

# Initialize FastAPI app - Unified version
app = FastAPI(
    title="IMP Document Processing System",
    description="Islamic Manpower Promoters - Smart Document Processing with AI OCR and Fallback",
    version="1.0.0-unified"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize components
config = Config()
logger = Logger(__name__)

# Smart initialization with automatic fallback
disable_ai_startup = os.getenv('DISABLE_AI_ON_STARTUP', 'false').lower() == 'true'

if disable_ai_startup:
    logger.info("üöÄ AI startup disabled - using basic processor for faster boot")
    document_processor = DocumentProcessor()
    ai_mode = False
    processor_version = "Basic Processing (Fast Startup)"
else:
    try:
        document_processor = EnhancedDocumentProcessor()
        ai_mode = True
        processor_version = "AI-Powered (Donut OCR)"
        logger.info("‚úÖ AI Document Processor (Donut OCR) initialized successfully")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è AI processor failed to initialize: {e}")
        logger.info("üîÑ Falling back to Basic Document Processor")
        document_processor = DocumentProcessor()
        ai_mode = False
        processor_version = "Basic Processing (Fallback)"

# Unified Pydantic models
class DocumentResponse(BaseModel):
    document_id: str
    original_filename: str
    quality_score: float
    processing_tier: str
    document_type: str
    status: str
    processing_cost: float
    requires_human_review: bool
    processing_time_seconds: Optional[float] = None
    ocr_engine: Optional[str] = None
    extracted_fields_count: Optional[int] = None
    confidence: Optional[float] = None
    ai_mode: bool = False

class ProcessingStats(BaseModel):
    total_processed: int
    high_quality_auto: int
    medium_quality_enhanced: int
    low_quality_manual: int
    processing_errors: int
    ai_processed: Optional[int] = None
    donut_ocr_processed: Optional[int] = None
    ai_mode_active: bool = False

class ExtractionResult(BaseModel):
    field_name: str
    value: str
    confidence: float
    source_region: Optional[dict] = None

class ExtractionResponse(BaseModel):
    document_id: str
    extracted_data: Dict[str, Any]
    extraction_results: List[ExtractionResult]
    overall_confidence: float
    processing_mode: str
    ai_engine_used: Optional[str] = None

@app.on_event("startup")
async def startup_event():
    """Initialize unified application on startup"""
    logger.info(f"üöÄ Starting IMP Document Processing System - {processor_version}")
    logger.info(f"üìä AI Mode: {'Enabled' if ai_mode else 'Disabled (Fallback)'}")
    
    # Initialize in-memory storage connection
    storage_connected = redis_client.connect()
    if storage_connected:
        logger.info("ÔøΩ In-memory storage initialized - Caching and queues enabled")
    else:
        logger.warning("‚ö†Ô∏è In-memory storage initialization failed")
    
    # Validate configuration
    validation_results = config.validate_config()
    validation_results['storage_connected'] = storage_connected
    logger.info("‚öôÔ∏è Configuration validation completed", validation_results)
    
    # Ensure directories exist
    os.makedirs(config.UPLOAD_FOLDER, exist_ok=True)
    os.makedirs(config.TEMP_FOLDER, exist_ok=True)
    os.makedirs(config.PROCESSED_FOLDER, exist_ok=True)
    logger.info("üìÅ Storage directories initialized")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    # Close in-memory storage connections
    redis_client.close()
    logger.info("ÔøΩ In-memory storage connections closed")
    
    if hasattr(document_processor, 'cleanup_resources'):
        document_processor.cleanup_resources()
    logger.info("üõë Application shutdown completed")

@app.get("/")
async def root():
    """Root endpoint with unified system information"""
    return {
        "service": "IMP Document Processing System",
        "version": "1.0.0-unified",
        "processor_mode": processor_version,
        "ai_enabled": ai_mode,
        "description": "Islamic Manpower Promoters - Smart Document Processing",
        "endpoints": {
            "upload": "/api/documents/upload",
            "extract": "/api/documents/extract",
            "status": "/api/documents/{id}/status",
            "stats": "/api/processing/stats",
            "health": "/api/health"
        },
        "features": [
            "Smart document classification",
            "Quality-based processing tiers",
            "AI-powered OCR (when available)",
            "Automatic fallback to basic processing",
            "Cost optimization",
            "Human-in-the-loop for low quality documents"
        ]
    }

@app.get("/api/health")
async def health_check():
    """Enhanced health check with storage and system status"""
    try:
        # Test document processor
        stats = document_processor.get_processing_stats()
        
        # Check AI availability
        ai_status = "available" if ai_mode else "unavailable (fallback active)"
        
        # Check storage status
        storage_connected = redis_client.is_connected()
        storage_status = "connected" if storage_connected else "disconnected"
        
        health_data = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "processor_mode": processor_version,
            "ai_mode": ai_mode,
            "ai_status": ai_status,
            "storage_connected": storage_connected,
            "storage_status": storage_status,
            "total_processed": stats.get('total_processed', 0),
            "error_rate": stats.get('processing_errors', 0) / max(stats.get('total_processed', 1), 1) * 100,
            "version": "1.0.0-unified"
        }
        
        # Add queue info if available
        if storage_connected:
            try:
                queue_lengths = redis_client.get_all_queue_lengths()
                health_data["queue_lengths"] = queue_lengths
                health_data["total_queued"] = sum(queue_lengths.values())
            except:
                health_data["queue_status"] = "limited_access"
        
        return health_data
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")

@app.post("/api/documents/upload", response_model=DocumentResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    enable_ocr: bool = Query(True, description="Enable OCR processing"),
    quality_threshold: Optional[float] = Query(None, description="Override quality threshold")
):
    """Unified document upload endpoint with smart processing"""
    try:
        logger.info(f"üì§ Processing upload: {file.filename} (AI: {ai_mode})")
        
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="Filename is required")
        
        # Check memory cache first (if available)
        file_hash = f"{file.filename}_{file.size}_{datetime.now().strftime('%Y%m%d')}"
        cached_result = redis_client.get_cached_document(file_hash)
        if cached_result:
            logger.info(f"üìã Returning cached result for {file.filename}")
            return DocumentResponse(**cached_result)
        
        # Save uploaded file
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:
            shutil.copyfileobj(file.file, temp_file)
            temp_path = temp_file.name
        
        try:
            # Process document with unified processor
            result = document_processor.process_document(
                temp_path, 
                original_filename=file.filename
            )
            
            # Enhanced response for AI mode
            response_data = {
                "document_id": result.id,
                "original_filename": result.original_filename,
                "quality_score": result.quality_metrics.overall_score if result.quality_metrics else 0,
                "processing_tier": result.processing_tier.value if result.processing_tier else "unknown",
                "document_type": result.document_type.value if result.document_type else "unknown",
                "status": result.status.value,
                "processing_cost": result.processing_cost,
                "requires_human_review": result.processing_tier == ProcessingTier.LOW_QUALITY if result.processing_tier else False,
                "processing_time_seconds": 0.0,  # Will be calculated in future versions
                "ai_mode": ai_mode
            }
            
            # Add AI-specific fields if available
            if hasattr(result, 'ocr_engine'):
                response_data["ocr_engine"] = result.ocr_engine
            if hasattr(result, 'extracted_fields_count'):
                response_data["extracted_fields_count"] = result.extracted_fields_count
            if hasattr(result, 'confidence'):
                response_data["confidence"] = result.confidence
                
            # Cache successful results in memory (if available)
            redis_client.cache_document_metadata(file_hash, response_data, ttl=3600)
            
            # Add to processing queue based on quality (if storage available)
            if result.processing_tier == ProcessingTier.HIGH_QUALITY:
                redis_client.add_to_queue('high_priority', {
                    'document_id': result.id,
                    'quality_score': response_data["quality_score"],
                    'processing_tier': 'high_quality'
                })
                redis_client.increment_counter('processing:high_quality')
            elif result.processing_tier == ProcessingTier.MEDIUM_QUALITY:
                redis_client.add_to_queue('medium_priority', {
                    'document_id': result.id,
                    'quality_score': response_data["quality_score"],
                    'processing_tier': 'medium_quality'
                })
                redis_client.increment_counter('processing:medium_quality')
            else:
                redis_client.add_to_queue('human_review', {
                    'document_id': result.id,
                    'quality_score': response_data["quality_score"],
                    'processing_tier': 'low_quality',
                    'requires_human_review': True
                })
                redis_client.increment_counter('processing:low_quality')
            
            # Update general counters
            redis_client.increment_counter('documents:processed')
                
            logger.info(f"‚úÖ Document processed successfully: {result.id}")
            return DocumentResponse(**response_data)
            
        finally:
            # Cleanup temp file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"‚ùå Upload processing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

@app.post("/api/documents/extract", response_model=ExtractionResponse)
async def extract_document_data(
    file: UploadFile = File(...),
    extract_fields: List[str] = Query(["name", "email", "phone", "experience"], description="Fields to extract")
):
    """Unified data extraction endpoint"""
    try:
        logger.info(f"üîç Extracting data from: {file.filename} (AI: {ai_mode})")
        
        # Save uploaded file
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as temp_file:
            shutil.copyfileobj(file.file, temp_file)
            temp_path = temp_file.name
        
        try:
            # Process with extraction
            if ai_mode and hasattr(document_processor, 'extract_structured_data'):
                # AI-powered extraction
                extraction_result = document_processor.extract_structured_data(
                    temp_path, 
                    extract_fields,
                    original_filename=file.filename
                )
                processing_mode = "AI-Powered Extraction"
                ai_engine = "donut_transformer"
            else:
                # Basic processing fallback
                result = document_processor.process_document(temp_path, original_filename=file.filename)
                extraction_result = {
                    "extracted_data": {"message": "Basic processing mode - limited extraction"},
                    "extraction_results": [],
                    "overall_confidence": 0.6,
                    "document_id": result.id
                }
                processing_mode = "Basic Processing"
                ai_engine = None
            
            return ExtractionResponse(
                document_id=extraction_result["document_id"],
                extracted_data=extraction_result["extracted_data"],
                extraction_results=extraction_result.get("extraction_results", []),
                overall_confidence=extraction_result.get("overall_confidence", 0.6),
                processing_mode=processing_mode,
                ai_engine_used=ai_engine
            )
            
        finally:
            # Cleanup temp file
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        logger.error(f"‚ùå Extraction failed: {e}")
        raise HTTPException(status_code=500, detail=f"Extraction failed: {str(e)}")

@app.get("/api/documents/{document_id}/status")
async def get_document_status(document_id: str):
    """Get processing status for a document"""
    try:
        # This would typically query a database
        # For now, return a basic response
        return {
            "document_id": document_id,
            "status": "completed",
            "processor_mode": processor_version,
            "ai_mode": ai_mode,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå Status check failed: {e}")
        raise HTTPException(status_code=404, detail=f"Document not found: {document_id}")

@app.get("/api/processing/stats", response_model=ProcessingStats)
async def get_processing_statistics():
    """Unified processing statistics"""
    try:
        stats = document_processor.get_processing_stats()
        
        response_data = {
            "total_processed": stats.get('total_processed', 0),
            "high_quality_auto": stats.get('high_quality_auto', 0),
            "medium_quality_enhanced": stats.get('medium_quality_enhanced', 0),
            "low_quality_manual": stats.get('low_quality_manual', 0),
            "processing_errors": stats.get('processing_errors', 0),
            "ai_mode_active": ai_mode
        }
        
        # Add AI-specific stats if available
        if ai_mode:
            response_data["ai_processed"] = stats.get('donut_ocr_processed', 0)
            response_data["donut_ocr_processed"] = stats.get('donut_ocr_processed', 0)
        
        return ProcessingStats(**response_data)
        
    except Exception as e:
        logger.error(f"‚ùå Stats retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"Stats unavailable: {str(e)}")

@app.get("/api/config")
async def get_configuration():
    """Get current system configuration"""
    return {
        "processor_mode": processor_version,
        "ai_enabled": ai_mode,
        "quality_thresholds": {
            "high": config.QUALITY_THRESHOLD_HIGH,
            "medium": config.QUALITY_THRESHOLD_MEDIUM,
            "low": config.QUALITY_THRESHOLD_LOW
        },
        "file_limits": {
            "max_size_mb": config.MAX_FILE_SIZE_MB
        },
        "storage": {
            "upload_folder": config.UPLOAD_FOLDER,
            "temp_folder": config.TEMP_FOLDER,
            "processed_folder": config.PROCESSED_FOLDER
        }
    }

@app.get("/api/storage/stats")
async def get_storage_stats():
    """Get in-memory storage statistics and queue information"""
    try:
        if not redis_client.is_connected():
            return {"message": "Storage not available", "stats": {}}
        
        # Get comprehensive storage statistics
        storage_stats = redis_client.get_processing_stats()
        storage_info = redis_client.get_redis_info()
        
        return {
            "storage_connected": True,
            "processing_stats": storage_stats,
            "storage_info": storage_info,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Storage stats failed: {e}")
        return {"storage_connected": False, "error": str(e)}

@app.get("/api/storage/queues")
async def get_queue_status():
    """Get current queue lengths and status"""
    try:
        if not redis_client.is_connected():
            return {"message": "Storage not available", "queues": {}}
        
        queue_lengths = redis_client.get_all_queue_lengths()
        
        return {
            "storage_connected": True,
            "queues": queue_lengths,
            "total_queued": sum(queue_lengths.values()),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Queue status failed: {e}")
        return {"storage_connected": False, "error": str(e)}

@app.get("/api/redis/queue/{queue_name}")
async def get_queue_details(queue_name: str):
    """Get specific queue details and pending items"""
    try:
        if not redis_client.is_connected():
            return {"message": "Redis not available"}
        
        queue_length = redis_client.get_queue_length(queue_name)
        
        return {
            "queue_name": queue_name,
            "length": queue_length,
            "redis_connected": True,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Queue details failed: {e}")
        return {"error": str(e), "redis_connected": False}

@app.post("/api/redis/queue/{queue_name}/process")
async def process_queue_item(queue_name: str):
    """Process next item from specified queue"""
    try:
        if not redis_client.is_connected():
            raise HTTPException(status_code=503, detail="Redis not available")
        
        # Get next item from queue
        item = redis_client.get_from_queue(queue_name)
        
        if not item:
            return {"message": f"No items in {queue_name} queue", "processed": False}
        
        # Process the item (basic implementation)
        document_id = item.get('document_id')
        logger.info(f"üîÑ Processing queue item: {document_id} from {queue_name}")
        
        # Update counters
        redis_client.increment_counter(f"queue:{queue_name}:processed")
        
        return {
            "message": f"Processed item from {queue_name}",
            "document_id": document_id,
            "item_data": item,
            "processed": True,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Queue processing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Queue processing failed: {str(e)}")

@app.delete("/api/redis/cache/{document_id}")
async def clear_document_cache(document_id: str):
    """Clear cached data for a specific document"""
    try:
        if not redis_client.is_connected():
            raise HTTPException(status_code=503, detail="Redis not available")
        
        # Clear document cache
        doc_deleted = redis_client.redis_client.delete(f"{redis_client.CACHE_PREFIXES['document']}{document_id}")
        quality_deleted = redis_client.redis_client.delete(f"{redis_client.CACHE_PREFIXES['quality']}{document_id}")
        
        return {
            "document_id": document_id,
            "cache_cleared": True,
            "items_deleted": doc_deleted + quality_deleted,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Cache clear failed: {e}")
        raise HTTPException(status_code=500, detail=f"Cache clear failed: {str(e)}")

# Run the application
if __name__ == "__main__":
    import uvicorn
    
    # Determine port and mode
    port = int(os.getenv("PORT", 8000))
    
    logger.info(f"üöÄ Starting unified IMP Document Processing System")
    logger.info(f"üìä Mode: {processor_version}")
    logger.info(f"üåê Server: http://localhost:{port}")
    logger.info(f"üìö API Docs: http://localhost:{port}/docs")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,  # Set to True for development
        log_level="info"
    )
